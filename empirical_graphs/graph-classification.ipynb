{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules.module import Module\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import random\n",
    "import networkx as nx\n",
    "from itertools import permutations, combinations\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "base_data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanAggregator(Module):\n",
    "    '''\n",
    "    This class does the sampling and aggregating given a set of features\n",
    "\n",
    "    We store everything except the set of nodes to aggregate on the class\n",
    "\n",
    "    We pass the nodes (batch, neg samples, etc) to forward\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        feature_dim,\n",
    "        emb_dim,\n",
    "        n_nbr_samples,\n",
    "        g,\n",
    "        dropout=0.5,\n",
    "        depth=1,\n",
    "        batchnorm=True,\n",
    "    ):\n",
    "        super(MeanAggregator, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.depth = depth\n",
    "        if batchnorm:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(feature_dim, emb_dim),\n",
    "                nn.BatchNorm1d(emb_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(dropout),\n",
    "            ).cuda()\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(feature_dim, emb_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(dropout),\n",
    "            ).cuda()\n",
    "        self.features = features\n",
    "        self.n_nbr_samples = n_nbr_samples\n",
    "        self.g = g # can be any dict like: {node: collection(nbrs)}\n",
    "        self.random_ = np.random.choice\n",
    "        self.set_ = set\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_list,\n",
    "        randomize_features=False,\n",
    "    ):\n",
    "        '''\n",
    "        features is (unique_node_dim by feature_dim)\n",
    "        mask is     (node_list by unique_node_dim)\n",
    "        '''\n",
    "        # samples and node_list are ordered the same\n",
    "        samples = [\n",
    "            list(self.random_(\n",
    "                list(self.g[node]),\n",
    "                self.n_nbr_samples,\n",
    "                replace=False,\n",
    "            )) + [node]\n",
    "            if len(self.g[node]) >= self.n_nbr_samples else list(self.g[node]) + [node]\n",
    "            for node in node_list\n",
    "        ]\n",
    "        unique_nodes_list = list(set.union(*(self.set_(x) for x in samples)))\n",
    "        # this helps us keep column indexes straight\n",
    "        unique_nodes_dict = {node: idx for idx, node in enumerate(unique_nodes_list)}\n",
    "\n",
    "        # rows: ordered by samples, cols: ordered by unique node idx (vals in unique_nodes_dict)\n",
    "        mask = torch.zeros(len(samples), len(unique_nodes_list)).cuda()\n",
    "\n",
    "        row_idxs = []\n",
    "        col_idxs = []\n",
    "        # rows are ordered in the same order as the batch\n",
    "        for node_idx, node_nbrs in enumerate(samples):\n",
    "            for alter in node_nbrs:\n",
    "                row_idxs.append(node_idx)\n",
    "                col_idxs.append(unique_nodes_dict[alter])\n",
    "        # for all but the outermost call, this self.features is a call to another encoder\n",
    "        # the smart thing about this design is we only get the 2nd hop nodes we need\n",
    "        sampled_features = self.fc(self.features(unique_nodes_list))\n",
    "        #if randomize_features: #  and self.depth > 1:\n",
    "        #    sampled_features = sampled_features[torch.randperm(sampled_features.size()[0])]\n",
    "        mask[row_idxs, col_idxs] = 1\n",
    "        mask = mask.div(mask.sum(dim=1).unsqueeze(1))\n",
    "        if randomize_features: #  and self.depth > 1:\n",
    "            mask = mask[torch.randperm(mask.size()[0])]\n",
    "        return mask.mm(sampled_features)\n",
    "\n",
    "\n",
    "class EncodingLayer(Module):\n",
    "    '''\n",
    "    Forward takes a batch and an aggregator\n",
    "    It runs one iter of the aggregator and then applies the encoding layer to it\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        feature_dim,\n",
    "        emb_input_dim,\n",
    "        emb_dim,\n",
    "        g,\n",
    "        agg,\n",
    "        base_model=None,\n",
    "        dropout=0.5,\n",
    "        depth=1,\n",
    "        batchnorm=True,\n",
    "    ):\n",
    "        super(EncodingLayer, self).__init__()\n",
    "        self.features = features\n",
    "        self.emb_dim = emb_dim\n",
    "        self.g = g\n",
    "        self.agg = agg\n",
    "        self.depth = depth\n",
    "        if base_model:\n",
    "            self.base_model = base_model\n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, emb_input_dim),\n",
    "            nn.BatchNorm1d(emb_input_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        ).cuda()\n",
    "        if batchnorm:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(emb_input_dim, emb_dim),\n",
    "                nn.BatchNorm1d(emb_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(dropout),\n",
    "            ).cuda()\n",
    "        else:\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(emb_input_dim, emb_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(dropout),\n",
    "            ).cuda()\n",
    "\n",
    "    def forward(self, node_list, randomize_features=False):\n",
    "        emb = self.agg(\n",
    "            node_list=node_list,\n",
    "            randomize_features=randomize_features,\n",
    "        )\n",
    "        # ego_features = self.features(node_list)\n",
    "        #if randomize_features and self.depth > 1:\n",
    "        #    ego_features = ego_features[torch.randperm(ego_features.size()[0])]\n",
    "        emb = self.fc(\n",
    "            # torch.cat((self.fc0(ego_features), emb), dim=1)\n",
    "            # self.fc0(ego_features) + emb\n",
    "            emb\n",
    "        )\n",
    "        return emb\n",
    "\n",
    "\n",
    "class MeanModel(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        n_nbr_samples1,\n",
    "        n_nbr_samples2,\n",
    "        g,\n",
    "        features,\n",
    "        hidden_dim=64,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(MeanModel, self).__init__()\n",
    "        feature_dim = features.size()[1]\n",
    "        self.agg1 = MeanAggregator(\n",
    "            features=lambda x: features[x],\n",
    "            feature_dim=feature_dim,\n",
    "            emb_dim=hidden_dim,\n",
    "            n_nbr_samples=n_nbr_samples1,\n",
    "            g=g,\n",
    "            dropout=dropout,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.enc1 = EncodingLayer(\n",
    "            features=lambda x: features[x],\n",
    "            feature_dim=feature_dim,\n",
    "            emb_input_dim=hidden_dim,\n",
    "            emb_dim=hidden_dim,\n",
    "            g=g,\n",
    "            agg=self.agg1,\n",
    "            base_model=None,\n",
    "            depth=2,\n",
    "            dropout=dropout,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.agg2 = MeanAggregator(\n",
    "            features=lambda x: self.enc1(x),\n",
    "            feature_dim=hidden_dim,\n",
    "            emb_dim=hidden_dim,\n",
    "            n_nbr_samples=n_nbr_samples2,\n",
    "            g=g,\n",
    "            dropout=dropout,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.enc2 = EncodingLayer(\n",
    "            features=lambda x: self.enc1(x),\n",
    "            feature_dim=hidden_dim,\n",
    "            emb_input_dim=hidden_dim,\n",
    "            emb_dim=emb_dim,\n",
    "            g=g,\n",
    "            agg=self.agg2,\n",
    "            base_model=self.enc1,\n",
    "            depth=1,\n",
    "            dropout=dropout,\n",
    "            batchnorm=True,\n",
    "        )\n",
    "        self.model = self.enc2.apply(init_weights)\n",
    "\n",
    "    def forward(self, node_list, randomize_features=False):\n",
    "        # return self.model(node_list, randomize_features)\n",
    "        if self.model.training:\n",
    "            return self.model(node_list, randomize_features)\n",
    "        else:\n",
    "            return torch.cat(\n",
    "                (\n",
    "                    self.enc2(node_list, False),\n",
    "                    self.enc1(node_list, False),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "    \n",
    "def run_model_within(\n",
    "    model_class,\n",
    "    emb_dim,\n",
    "    n_nbr_samples1,\n",
    "    n_nbr_samples2,\n",
    "    n_pos_samples,\n",
    "    n_neg_samples_rand,\n",
    "    n_neg_samples_shuffle,\n",
    "    g,\n",
    "    features,\n",
    "    graph_label_dict,\n",
    "    lr=0.01,\n",
    "    n_runs=20,\n",
    "    n_epochs=20,\n",
    "    dropout=0.5,\n",
    "    batch_size=256,\n",
    "):\n",
    "    accs = []\n",
    "    aucs = []\n",
    "    # accs_sd = []\n",
    "\n",
    "    node_list = [x for x in g.node]\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        model1 = model_class(\n",
    "            emb_dim=emb_dim,\n",
    "            n_nbr_samples1=n_nbr_samples1,\n",
    "            n_nbr_samples2=n_nbr_samples2,\n",
    "            g=g,\n",
    "            features=features,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        optimizer1 = optim.Adam(\n",
    "            model1.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "        model2 = model_class(\n",
    "            emb_dim=emb_dim,\n",
    "            n_nbr_samples1=n_nbr_samples1,\n",
    "            n_nbr_samples2=n_nbr_samples2,\n",
    "            g=g,\n",
    "            features=features,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        optimizer2 = optim.Adam(\n",
    "            model2.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "        total_loss = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            model1 = model1.train()\n",
    "            model2 = model2.train()\n",
    "            random.shuffle(node_list)\n",
    "            batch = node_list[:batch_size]\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "            emb_u = model1(batch)\n",
    "            nbrs = []\n",
    "            for node in batch:\n",
    "                for _ in range(n_pos_samples):\n",
    "                    nbrs.append(node)\n",
    "                #for _ in range(n_pos_samples):\n",
    "                #    nbrs.append(random.choice(list(g[node])))\n",
    "            emb_v = model2(nbrs).view(n_pos_samples * len(batch), -1)\n",
    "            neg_nodes_shuffle = []\n",
    "            neg_nodes_rand = []\n",
    "            for idx, node in enumerate(batch):\n",
    "                for _ in range(n_neg_samples_shuffle):\n",
    "                    neg_nodes_shuffle.append(node)\n",
    "                batch_minus_ego = list(set(batch) - {node}) #  - set(list(g[node])))\n",
    "                for _ in range(n_neg_samples_rand):\n",
    "                    neg_nodes_rand.append(\n",
    "                        random.choice(\n",
    "                            batch_minus_ego\n",
    "                        )\n",
    "                    )\n",
    "            if len(neg_nodes_shuffle) > 0 and len(neg_nodes_rand) > 0:\n",
    "                emb_neg1 = model2(neg_nodes_shuffle, randomize_features=True)\n",
    "                emb_neg2 = model2(neg_nodes_rand, randomize_features=False)\n",
    "                total_neg_samples = n_neg_samples_rand + n_neg_samples_shuffle\n",
    "                emb_neg = torch.cat((emb_neg1, emb_neg2), dim=1).view(\n",
    "                    total_neg_samples * len(batch),\n",
    "                    -1,\n",
    "                )\n",
    "            elif len(neg_nodes_shuffle) > 0 and len(neg_nodes_rand) == 0:\n",
    "                emb_neg = model2(neg_nodes_shuffle, randomize_features=True)\n",
    "            elif len(neg_nodes_shuffle) == 0 and len(neg_nodes_rand) > 0:\n",
    "                emb_neg = model2(neg_nodes_rand, randomize_features=False)\n",
    "            pos_weight = emb_neg.numel() / emb_u.numel()\n",
    "            loss = sigmoid_loss(emb_u, emb_v, emb_neg, pos_weight)\n",
    "            total_loss += float(loss.cpu().data.numpy())\n",
    "            # print(total_loss / (epoch + 1))\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "        model1 = model1.eval()\n",
    "        \n",
    "        graph_nodes = defaultdict(list)\n",
    "\n",
    "        for node in g.node:\n",
    "            graph_node = g.node[node]['graph_idx']\n",
    "            graph_nodes[graph_node].append(node)\n",
    "\n",
    "        arr_list = []\n",
    "\n",
    "        for g_idx, nodes in graph_nodes.items():\n",
    "            g_emb = model1(nodes).cpu().data.numpy()\n",
    "            arr_list.append(g_emb)\n",
    "\n",
    "        y = []\n",
    "        X = []\n",
    "\n",
    "        for g_idx, g_arr in zip(graph_nodes.keys(), arr_list):\n",
    "            y.append(graph_label_dict[g_idx])\n",
    "            X.append(g_arr.sum(axis=0))\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        run_aucs = []\n",
    "        run_accs = []\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            if len(set(y)) > 2:\n",
    "                logit = OneVsRestClassifier(LogisticRegression(penalty='l2'))\n",
    "                logit.fit(X_train, y_train)\n",
    "                preds = logit.predict(X_test)\n",
    "                run_aucs.append(0)\n",
    "                run_accs.append(\n",
    "                    accuracy_score(y_test, preds)\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logit = LogisticRegression(penalty='l2')\n",
    "                logit.fit(X_train, y_train)\n",
    "\n",
    "                train_preds = logit.predict_proba(X_train)[:,1]\n",
    "                fpr, tpr, thresholds = roc_curve(y_train, train_preds)\n",
    "                max_acc = 0\n",
    "                max_acc_threshold = 0\n",
    "                #max_j = 0\n",
    "                #max_j_threshold = 0\n",
    "                for th in thresholds:\n",
    "                    cm = confusion_matrix(y_train, train_preds > th)\n",
    "                    # print(cm)\n",
    "                    #TP = cm[0,0]\n",
    "                    #FP = cm[0,1]\n",
    "                    #FN = cm[1,0]\n",
    "                    #TN = cm[1,1]\n",
    "                    #j = TP / (TP + FN) + TN / (TN + FP) - 1\n",
    "                    #if j > max_j:\n",
    "                    #    max_j_threshold = th\n",
    "                    #    max_j = j\n",
    "                    train_acc = accuracy_score(y_train, train_preds > th)\n",
    "                    if train_acc > max_acc:\n",
    "                        max_acc = train_acc\n",
    "                        max_acc_threshold = th\n",
    "                preds = logit.predict_proba(X_test)[:,1]\n",
    "                run_aucs.append(roc_auc_score(y_test, preds))\n",
    "                run_accs.append(\n",
    "                    accuracy_score(y_test, preds > max_acc_threshold)\n",
    "                )\n",
    "        print(np.mean(run_accs))\n",
    "        print(np.mean(run_aucs))\n",
    "        accs.append(run_accs)\n",
    "        aucs.append(run_aucs)\n",
    "    return accs, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_imdb_b():\n",
    "    g = nx.Graph()\n",
    "    with open(base_data_path + 'IMDB-BINARY/IMDB-BINARY_A.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = [int(x) for x in line.strip().split(', ')]\n",
    "            g.add_edge(n1, n2)\n",
    "    with open(base_data_path + 'IMDB-BINARY/IMDB-BINARY_graph_indicator.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            graph_idx = int(line.strip())\n",
    "            g.node[node_idx_one]['graph_idx'] = graph_idx\n",
    "    g = nx.convert_node_labels_to_integers(g)\n",
    "    graph_label_dict = {}\n",
    "    with open(base_data_path + 'IMDB-BINARY/IMDB-BINARY_graph_labels.txt', 'r') as f:\n",
    "        for graph_idx, line in enumerate(f):\n",
    "            graph_idx_one = graph_idx + 1\n",
    "            graph_label_dict[graph_idx_one] = 1 if int(line.strip()) == 1 else 0\n",
    "    structural_features = []\n",
    "    feature_dim = 30\n",
    "    for node in g.node:\n",
    "        structural_features.append(\n",
    "            [g.degree(node)] + sorted(pad_features([g.degree(x) for x in g[node]], feature_dim), reverse=True)\n",
    "        )\n",
    "    structural_features = torch.FloatTensor(structural_features)\n",
    "    return g, structural_features, None, graph_label_dict\n",
    "\n",
    "\n",
    "def gen_mutag():\n",
    "    g = nx.Graph()\n",
    "    with open(base_data_path + 'MUTAG/MUTAG_A.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = [int(x) for x in line.strip().split(', ')]\n",
    "            g.add_edge(n1, n2)\n",
    "    with open(base_data_path + 'MUTAG/MUTAG_graph_indicator.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            graph_idx = int(line.strip())\n",
    "            g.node[node_idx_one]['graph_idx'] = graph_idx\n",
    "    with open(base_data_path + 'MUTAG/MUTAG_node_labels.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            node_label = [0., 0., 0., 0., 0., 0., 0.]\n",
    "            node_label[int(line.strip())] = 1.0\n",
    "            g.node[node_idx_one]['node_label'] = node_label\n",
    "    g = nx.convert_node_labels_to_integers(g)\n",
    "    graph_label_dict = {}\n",
    "    with open(base_data_path + 'MUTAG/MUTAG_graph_labels.txt', 'r') as f:\n",
    "        for graph_idx, line in enumerate(f):\n",
    "            graph_idx_one = graph_idx + 1\n",
    "            graph_label_dict[graph_idx_one] = 1 if int(line.strip()) == 1 else 0\n",
    "    structural_features = []\n",
    "    feature_dim = max(dict(g.degree).values())\n",
    "    for node in g.node:\n",
    "        structural_features.append(\n",
    "            [g.degree(node)] + sorted(pad_features([g.degree(x) for x in g[node]], feature_dim), reverse=True)\n",
    "        )\n",
    "    structural_features = torch.FloatTensor(structural_features)\n",
    "    other_features = []\n",
    "    for node in g.node:\n",
    "        other_features.append(g.node[node]['node_label'])\n",
    "    other_features = torch.FloatTensor(other_features)\n",
    "    return g, structural_features, other_features, graph_label_dict\n",
    "\n",
    "\n",
    "def gen_reddit_b():\n",
    "    g = nx.Graph()\n",
    "    graph_label_dict = {}\n",
    "    with open(base_data_path + 'REDDIT-BINARY/REDDIT-BINARY_A.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = [int(x) for x in line.strip().split(', ')]\n",
    "            g.add_edge(n1, n2)\n",
    "    with open(base_data_path + 'REDDIT-BINARY/REDDIT-BINARY_graph_indicator.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            graph_idx = int(line.strip())\n",
    "            if node_idx_one in g:\n",
    "                g.node[node_idx_one]['graph_idx'] = graph_idx\n",
    "    g = nx.convert_node_labels_to_integers(g)\n",
    "    with open(base_data_path + 'REDDIT-BINARY/REDDIT-BINARY_graph_labels.txt', 'r') as f:\n",
    "        for graph_idx, line in enumerate(f):\n",
    "            graph_idx_one = graph_idx + 1\n",
    "            graph_label_dict[graph_idx_one] = 1 if int(line.strip()) == 1 else 0\n",
    "    structural_features = []\n",
    "    feature_dim = 30\n",
    "    for node in g.node:\n",
    "        structural_features.append(\n",
    "            [g.degree(node)] + sorted(pad_features([g.degree(x) for x in g[node]], feature_dim), reverse=True)\n",
    "        )\n",
    "    structural_features = torch.FloatTensor(structural_features)\n",
    "    return g, structural_features, None, graph_label_dict\n",
    "\n",
    "\n",
    "def gen_imdb_m():\n",
    "    g = nx.Graph()\n",
    "    with open(base_data_path + 'IMDB-MULTI/IMDB-MULTI_A.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = [int(x) for x in line.strip().split(', ')]\n",
    "            g.add_edge(n1, n2)\n",
    "    with open(base_data_path + 'IMDB-MULTI/IMDB-MULTI_graph_indicator.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            graph_idx = int(line.strip())\n",
    "            g.node[node_idx_one]['graph_idx'] = graph_idx\n",
    "    g = nx.convert_node_labels_to_integers(g)\n",
    "    graph_label_dict = {}\n",
    "    with open(base_data_path + 'IMDB-MULTI/IMDB-MULTI_graph_labels.txt', 'r') as f:\n",
    "        for graph_idx, line in enumerate(f):\n",
    "            graph_idx_one = graph_idx + 1\n",
    "            graph_label_dict[graph_idx_one] = int(line.strip())\n",
    "    structural_features = []\n",
    "    feature_dim = 30\n",
    "    for node in g.node:\n",
    "        structural_features.append(\n",
    "            [g.degree(node)] + sorted(pad_features([g.degree(x) for x in g[node]], feature_dim), reverse=True)\n",
    "        )\n",
    "    structural_features = torch.FloatTensor(structural_features)\n",
    "    return g, structural_features, None, graph_label_dict\n",
    "\n",
    "\n",
    "def gen_reddit_m5k():\n",
    "    g = nx.Graph()\n",
    "    with open(base_data_path + 'REDDIT-MULTI-5K/REDDIT-MULTI-5K_A.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = [int(x) for x in line.strip().split(', ')]\n",
    "            g.add_edge(n1, n2)\n",
    "    with open(base_data_path + 'REDDIT-MULTI-5K/REDDIT-MULTI-5K_graph_indicator.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            graph_idx = int(line.strip())\n",
    "            if node_idx_one not in g:\n",
    "                g.add_node(node_idx_one)\n",
    "                g.add_edge(node_idx_one, random.choice(list(g.node)))\n",
    "            g.node[node_idx_one]['graph_idx'] = graph_idx\n",
    "    g = nx.convert_node_labels_to_integers(g)\n",
    "    graph_label_dict = {}\n",
    "    with open(base_data_path + 'REDDIT-MULTI-5K/REDDIT-MULTI-5K_graph_labels.txt', 'r') as f:\n",
    "        for graph_idx, line in enumerate(f):\n",
    "            graph_idx_one = graph_idx + 1\n",
    "            graph_label_dict[graph_idx_one] = int(line.strip())\n",
    "    structural_features = []\n",
    "    feature_dim = 30\n",
    "    for node in g.node:\n",
    "        structural_features.append(\n",
    "            [g.degree(node)] + sorted(pad_features([g.degree(x) for x in g[node]], feature_dim), reverse=True)\n",
    "        )\n",
    "    structural_features = torch.FloatTensor(structural_features)\n",
    "    return g, structural_features, None, graph_label_dict\n",
    "\n",
    "\n",
    "def gen_reddit_m12k():\n",
    "    g = nx.Graph()\n",
    "    with open(base_data_path + 'REDDIT-MULTI-12K/REDDIT-MULTI-12K_A.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            n1, n2 = [int(x) for x in line.strip().split(', ')]\n",
    "            g.add_edge(n1, n2)\n",
    "    with open(base_data_path + 'REDDIT-MULTI-12K/REDDIT-MULTI-12K_graph_indicator.txt', 'r') as f:\n",
    "        for node_idx, line in enumerate(f):\n",
    "            node_idx_one = node_idx + 1\n",
    "            graph_idx = int(line.strip())\n",
    "            if node_idx_one not in g:\n",
    "                g.add_node(node_idx_one)\n",
    "                g.add_edge(node_idx_one, random.choice(list(g.node)))\n",
    "            g.node[node_idx_one]['graph_idx'] = graph_idx\n",
    "    g = nx.convert_node_labels_to_integers(g)\n",
    "    graph_label_dict = {}\n",
    "    with open(base_data_path + 'REDDIT-MULTI-12K/REDDIT-MULTI-12K_graph_labels.txt', 'r') as f:\n",
    "        for graph_idx, line in enumerate(f):\n",
    "            graph_idx_one = graph_idx + 1\n",
    "            graph_label_dict[graph_idx_one] = int(line.strip())\n",
    "    structural_features = []\n",
    "    feature_dim = 30\n",
    "    for node in g.node:\n",
    "        structural_features.append(\n",
    "            [g.degree(node)] + sorted(pad_features([g.degree(x) for x in g[node]], feature_dim), reverse=True)\n",
    "        )\n",
    "    structural_features = torch.FloatTensor(structural_features)\n",
    "    return g, structural_features, None, graph_label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g, features, action_features, graph_label_dict = gen_mutag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutag_within_accs, mutag_within_aucs = run_model_within(\n",
    "    MeanModel,\n",
    "    emb_dim=64,\n",
    "    n_nbr_samples1=4,\n",
    "    n_nbr_samples2=4,\n",
    "    n_pos_samples=5,\n",
    "    n_neg_samples_rand=10,\n",
    "    n_neg_samples_shuffle=10,\n",
    "    g=g,\n",
    "    features=torch.cat((action_features, features), dim=1).cuda(),\n",
    "    graph_label_dict=graph_label_dict,\n",
    "    lr=0.02,\n",
    "    n_runs=30,\n",
    "    n_epochs=20,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutag_means = [np.mean(x) for x in mutag_within_accs]\n",
    "mutag_aucs = [np.mean(x) for x in mutag_within_aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mutag_means), np.std(mutag_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mutag_aucs), np.std(mutag_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g, features, action_features, graph_label_dict = gen_imdb_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_b_within_accs, imdb_b_within_aucs = run_model_within(\n",
    "    MeanModel,\n",
    "    emb_dim=64,\n",
    "    n_nbr_samples1=4,\n",
    "    n_nbr_samples2=4,\n",
    "    n_pos_samples=5,\n",
    "    n_neg_samples_rand=10,\n",
    "    n_neg_samples_shuffle=10,\n",
    "    g=g,\n",
    "    features=features.cuda(),\n",
    "    graph_label_dict=graph_label_dict,\n",
    "    lr=0.02,\n",
    "    n_runs=30,\n",
    "    n_epochs=0,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(imdb_b_within_accs))\n",
    "print(np.mean(imdb_b_within_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_b_means = [np.mean(x) for x in imdb_b_within_accs]\n",
    "imdb_b_aucs = [np.mean(x) for x in imdb_b_within_aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(imdb_b_means), np.std(imdb_b_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(imdb_b_aucs), np.std(imdb_b_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g, features, _, graph_label_dict = gen_reddit_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_b_within_accs, reddit_b_within_aucs = run_model_within(\n",
    "    MeanModel,\n",
    "    emb_dim=64,\n",
    "    n_nbr_samples1=4,\n",
    "    n_nbr_samples2=4,\n",
    "    n_pos_samples=5,\n",
    "    n_neg_samples_rand=10,\n",
    "    n_neg_samples_shuffle=10,\n",
    "    g=g,\n",
    "    features=features.cuda(),\n",
    "    graph_label_dict=graph_label_dict,\n",
    "    lr=0.02,\n",
    "    n_runs=20,\n",
    "    n_epochs=0,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(reddit_b_within_accs))\n",
    "print(np.mean(reddit_b_within_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_b_means = [np.mean(x) for x in reddit_b_within_accs]\n",
    "reddit_b_aucs = [np.mean(x) for x in reddit_b_within_aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(reddit_b_means), np.std(reddit_b_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(reddit_b_aucs), np.std(reddit_b_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g, features, _, graph_label_dict = gen_imdb_m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_m_within_accs, imdb_m_within_aucs = run_model_within(\n",
    "    MeanModel,\n",
    "    emb_dim=64,\n",
    "    n_nbr_samples1=25,\n",
    "    n_nbr_samples2=10,\n",
    "    n_pos_samples=5,\n",
    "    n_neg_samples_rand=10,\n",
    "    n_neg_samples_shuffle=10,\n",
    "    g=g,\n",
    "    features=features.cuda(),\n",
    "    graph_label_dict=graph_label_dict,\n",
    "    lr=0.001,\n",
    "    n_runs=20,\n",
    "    n_epochs=0,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(imdb_m_within_accs))\n",
    "print(np.mean(imdb_m_within_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_m_means = [np.mean(x) for x in imdb_m_within_accs]\n",
    "imdb_m_aucs = [np.mean(x) for x in imdb_m_within_aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.506 0.0059479221395187945\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(imdb_m_means), np.std(imdb_m_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(imdb_m_aucs), np.std(imdb_m_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reddit m5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g, features, _, graph_label_dict = gen_reddit_m5k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_m5k_within_accs, reddit_m5k_within_aucs = run_model_within(\n",
    "    MeanModel,\n",
    "    emb_dim=64,\n",
    "    n_nbr_samples1=25,\n",
    "    n_nbr_samples2=10,\n",
    "    n_pos_samples=5,\n",
    "    n_neg_samples_rand=10,\n",
    "    n_neg_samples_shuffle=10,\n",
    "    g=g,\n",
    "    features=features.cuda(),\n",
    "    graph_label_dict=graph_label_dict,\n",
    "    lr=0.02,\n",
    "    n_runs=20,\n",
    "    n_epochs=0,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(reddit_m5k_within_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_m5k_means = [np.mean(x) for x in reddit_m5k_within_accs]\n",
    "reddit_m5k_aucs = [np.mean(x) for x in reddit_m5k_within_aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(reddit_m5k_means), np.std(reddit_m5k_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(reddit_m5k_aucs), np.std(reddit_m5k_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit m12k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g, features, _, graph_label_dict = gen_reddit_m12k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_m12k_within_accs, reddit_m12k_within_aucs = run_model_within(\n",
    "    MeanModel,\n",
    "    emb_dim=64,\n",
    "    n_nbr_samples1=25,\n",
    "    n_nbr_samples2=10,\n",
    "    n_pos_samples=4,\n",
    "    n_neg_samples_rand=10,\n",
    "    n_neg_samples_shuffle=10,\n",
    "    g=g,\n",
    "    features=features.cuda(),\n",
    "    graph_label_dict=graph_label_dict,\n",
    "    lr=0.02,\n",
    "    n_runs=20,\n",
    "    n_epochs=0,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(reddit_m12k_within_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_m12k_means = [np.mean(x) for x in reddit_m12k_within_accs]\n",
    "reddit_m12k_aucs = [np.mean(x) for x in reddit_m12k_within_aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(reddit_m12k_means), np.std(reddit_m12k_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(reddit_m12k_aucs), np.std(reddit_m12k_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
